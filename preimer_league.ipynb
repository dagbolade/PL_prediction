{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Premier League 2023-2024 prediction"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69178bb516e1d6d3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93e4c314e429ce32"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Importing the dataset and creating a dataframe\n",
    "df =  pd.read_excel('all-euro-data-2023-2024.xlsx', sheet_name='E0' )\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3645d634c5813470"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# convert the dataframe to csv file\n",
    "df.to_csv('premier_league.csv')\n",
    "df_new = pd.read_csv('premier_league.csv')\n",
    "df_new.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b9720e92d7afb151"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# removing the first column\n",
    "df_new.drop(df_new.columns[0], axis=1, inplace=True)\n",
    "df_new.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e12588df19ee2296"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the new dataframe to csv file \n",
    "df_new.to_csv('premier_league.csv')\n",
    "df_new"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d9cfa92b7850716a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db37ef466ab4489d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# checking for missing values\n",
    "df_new.isnull().sum()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b00d6e40ebbadbd8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# checking for duplicates\n",
    "df_new.duplicated().sum()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21ea8c731cb01e69"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# checking for outliers\n",
    "import matplotlib.pyplot as plt\n",
    "df_new.boxplot(figsize=(12,8))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f41dbf77108e3cb5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Performing Exploratory Data Analysis"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da1a69d61131ea30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# checking the total matches played\n",
    "total_matches = df_new.shape[0]\n",
    "print('Total matches played:', total_matches)\n",
    "\n",
    "#checking the list of all teams involved\n",
    "teams = df_new['HomeTeam'].unique()\n",
    "print('Teams involved:', teams)\n",
    "\n",
    "# checking the total number of goals scored\n",
    "total_goals = df_new['FTHG'].sum() + df_new['FTAG'].sum()\n",
    "print('Total goals scored so far:', total_goals)\n",
    "\n",
    "# checking the total number of home wins for chelsea\n",
    "chelsea_home_wins = df_new[(df_new['HomeTeam'] == 'Chelsea') & (df_new['FTR'] == 'H')].shape[0]\n",
    "print('Total number of home wins for Chelsea:', chelsea_home_wins)\n",
    "\n",
    "# checking the total number of home losses for manchester united\n",
    "man_united_home_losses = df_new[(df_new['HomeTeam'] == 'Man United') & (df_new['FTR'] == 'A')].shape[0]\n",
    "print('Total number of home losses for this shitty man utd team is: ', man_united_home_losses)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ddd3d84c9793c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get the current position and calculate points for chelsea and the rest of the teams\n",
    "def calculate_points_goals(row):\n",
    "    home_points, away_points = 0, 0\n",
    "    home_goal_diff = row['FTHG'] - row['FTAG']\n",
    "    away_goal_diff = row['FTAG'] - row['FTHG']\n",
    "\n",
    "    if row['FTR'] == 'H':\n",
    "        home_points = 3\n",
    "    elif row['FTR'] == 'A':\n",
    "        away_points = 3\n",
    "    else:\n",
    "        home_points = away_points = 1\n",
    "\n",
    "    return pd.Series([home_points, away_points, home_goal_diff, away_goal_diff, row['FTHG'], row['FTAG']])\n",
    "\n",
    "# Applying the function to the dataset\n",
    "df_new[['HomePoints', 'AwayPoints', 'HomeGoalDiff', 'AwayGoalDiff', 'HomeGoals', 'AwayGoals']] = df_new.apply(calculate_points_goals, axis=1)\n",
    "\n",
    "# Summarizing the data for each team\n",
    "team_stats = pd.DataFrame(index=teams)\n",
    "\n",
    "# Calculating total points, goal difference, and goals scored for each team\n",
    "team_stats['Points'] = df_new.groupby('HomeTeam')['HomePoints'].sum() + df_new.groupby('AwayTeam')['AwayPoints'].sum()\n",
    "team_stats['GoalDiff'] = df_new.groupby('HomeTeam')['HomeGoalDiff'].sum() + df_new.groupby('AwayTeam')['AwayGoalDiff'].sum()\n",
    "team_stats['GoalsScored'] = df_new.groupby('HomeTeam')['HomeGoals'].sum() + df_new.groupby('AwayTeam')['AwayGoals'].sum()\n",
    "\n",
    "# Sorting the teams based on Points, Goal Difference, and Goals Scored\n",
    "sorted_teams = team_stats.sort_values(by=['Points', 'GoalDiff', 'GoalsScored'], ascending=[False, False, False])\n",
    "\n",
    "# Finding Chelsea's position\n",
    "chelsea_position_updated = sorted_teams.index.get_loc('Chelsea') + 1\n",
    "print('Chelsea is currently in position:', chelsea_position_updated)\n",
    "sorted_teams.head(), chelsea_position_updated\n",
    "\n",
    "\n",
    "   "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "435d3e8d6c882933"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analysing Chelsea's performance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67d8b118fc7f9eb9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# team performance overview\n",
    "Total_matches = df_new[(df_new['HomeTeam'] == 'Chelsea') | (df_new['AwayTeam'] == 'Chelsea')].shape[0]\n",
    "Total_matches\n",
    "\n",
    "# total number of goals scored by chelsea\n",
    "Total_goals_scored = df_new[(df_new['HomeTeam']== 'Chelsea')]['FTHG'].sum() + df_new[(df_new['HomeTeam'] == 'Chelsea')]['FTAG'].sum()\n",
    "Total_goals_scored\n",
    "\n",
    "# total number of goals conceded by chelsea\n",
    "Total_goals_conceded = df_new[(df_new['HomeTeam'] == 'Chelsea')]['FTAG'].sum() + df_new[(df_new['AwayTeam'] == 'Chelsea')]['FTHG'].sum()\n",
    "Total_goals_conceded\n",
    "\n",
    "# total number of home wins for chelsea\n",
    "Total_home_wins = df_new[(df_new['HomeTeam'] == 'Chelsea') & (df_new['FTR'] == 'H')].shape[0]\n",
    "\n",
    "# total number of home losses for chelsea\n",
    "Total_home_losses = df_new[(df_new['HomeTeam'] == 'Chelsea') & (df_new['FTR'] == 'A')].shape[0]\n",
    "\n",
    "# total number of away wins for chelsea\n",
    "Total_away_wins = df_new[(df_new['AwayTeam'] == 'Chelsea') & (df_new['FTR'] == 'A')].shape[0]\n",
    "\n",
    "# total number of away losses for chelsea\n",
    "Total_away_losses = df_new[(df_new['AwayTeam'] == 'Chelsea') & (df_new['FTR'] == 'H')].shape[0]\n",
    "\n",
    "# total number of home draws for chelsea\n",
    "Total_home_draws = df_new[(df_new['HomeTeam'] == 'Chelsea') & (df_new['FTR'] == 'D')].shape[0]\n",
    "\n",
    "# total number of away draws for chelsea\n",
    "Total_away_draws = df_new[(df_new['AwayTeam'] == 'Chelsea') & (df_new['FTR'] == 'D')].shape[0]\n",
    "\n",
    "print(f'Chelsea has played a total of {Total_matches} matches so far in the 2023-2024 season')\n",
    "print(f'Chelsea has scored a total of {Total_goals_scored} goals so far in the 2023-2024 season')\n",
    "print(f'Chelsea has conceded a total of {Total_goals_conceded} goals so far in the 2023-2024 season')\n",
    "print(f'Chelsea has won a total of {Total_home_wins} home matches so far in the 2023-2024 season')\n",
    "print(f'Chelsea has lost a total of {Total_home_losses} home matches so far in the 2023-2024 season')\n",
    "print(f'Chelsea has won a total of {Total_away_wins} away matches so far in the 2023-2024 season')\n",
    "print(f'Chelsea has lost a total of {Total_away_losses} away matches so far in the 2023-2024 season')\n",
    "print(f'Chelsea has drawn a total of {Total_home_draws} home matches so far in the 2023-2024 season')\n",
    "print(f'Chelsea has drawn a total of {Total_away_draws} away matches so far in the 2023-2024 season')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cdb7d73038ce869"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Analysing the top 6 teams"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b669370999585bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# top 6 teams\n",
    "top_6_teams = sorted_teams.head(6)\n",
    "top_6_teams"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd888eeb26c09346"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### feature engineering"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b8e274bb242e32c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# selecting the features for the model\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encoding Team Names\n",
    "label_encoder = LabelEncoder()\n",
    "df_new['HomeTeamEncoded'] = label_encoder.fit_transform(df_new['HomeTeam'])\n",
    "df_new['AwayTeamEncoded'] = label_encoder.transform(df_new['AwayTeam'])\n",
    "\n",
    "# Calculate Team's Average Goals Scored and Conceded per Match\n",
    "# Replace 'FTHG' and 'FTAG' with your actual column names for goals\n",
    "average_goals_scored_home = df_new.groupby('HomeTeam')['FTHG'].mean()\n",
    "average_goals_scored_away = df_new.groupby('AwayTeam')['FTAG'].mean()\n",
    "\n",
    "# Mapping these averages to the main DataFrame\n",
    "df_new['HomeTeamAvgGoals'] = df_new['HomeTeam'].map(average_goals_scored_home)\n",
    "df_new['AwayTeamAvgGoals'] = df_new['AwayTeam'].map(average_goals_scored_away)\n",
    "\n",
    "# Viewing the modified DataFrame\n",
    "print(df_new.head())\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "594faf549ec6833f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate Home Team Win Percentage\n",
    "home_team_wins = df_new[df_new['FTR'] == 'H'].groupby('HomeTeam').size()\n",
    "total_home_matches = df_new.groupby('HomeTeam').size()\n",
    "home_team_win_percentage = home_team_wins / total_home_matches\n",
    "\n",
    "# Calculate Away Team Win Percentage\n",
    "away_team_wins = df_new[df_new['FTR'] == 'A'].groupby('AwayTeam').size()\n",
    "total_away_matches = df_new.groupby('AwayTeam').size()\n",
    "away_team_win_percentage = away_team_wins / total_away_matches\n",
    "\n",
    "# Create mappings for win percentages\n",
    "home_team_win_percentage_map = home_team_win_percentage.to_dict()\n",
    "away_team_win_percentage_map = away_team_win_percentage.to_dict()\n",
    "\n",
    "# Map the win percentages to the original DataFrame\n",
    "df_new['HomeTeamWinPercentage'] = df_new['HomeTeam'].map(home_team_win_percentage_map).fillna(0)\n",
    "df_new['AwayTeamWinPercentage'] = df_new['AwayTeam'].map(away_team_win_percentage_map).fillna(0)\n",
    "\n",
    "# Check the first few rows to confirm the new features\n",
    "print(df_new[['HomeTeam', 'HomeTeamWinPercentage', 'AwayTeam', 'AwayTeamWinPercentage']].head())\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57ebec12b7a216fb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Ensure the DataFrame is sorted by date\n",
    "df_new.sort_values('Date', inplace=True)\n",
    "\n",
    "# Initialize columns for points and form\n",
    "df_new['HomeTeamPoints'] = 0\n",
    "df_new['AwayTeamPoints'] = 0\n",
    "df_new['HomeTeamRecentForm'] = 0\n",
    "df_new['AwayTeamRecentForm'] = 0\n",
    "\n",
    "# Calculate points for each team\n",
    "for team in df_new['HomeTeam'].unique():\n",
    "    # Calculate points for the home team\n",
    "    home_points = df_new[df_new['HomeTeam'] == team]['FTR'].apply(lambda x: 3 if x == 'H' else 1 if x == 'D' else 0).cumsum()\n",
    "    away_points = df_new[df_new['AwayTeam'] == team]['FTR'].apply(lambda x: 3 if x == 'A' else 1 if x == 'D' else 0).cumsum()\n",
    "\n",
    "    # Assign the points to the team's home and away matches\n",
    "    df_new.loc[df_new['HomeTeam'] == team, 'HomeTeamPoints'] = home_points\n",
    "    df_new.loc[df_new['AwayTeam'] == team, 'AwayTeamPoints'] = away_points\n",
    "\n",
    "# Calculate recent form for each team\n",
    "for team in df_new['HomeTeam'].unique():\n",
    "    # Get indices of the team's home and away games\n",
    "    home_indices = df_new[df_new['HomeTeam'] == team].index\n",
    "    away_indices = df_new[df_new['AwayTeam'] == team].index\n",
    "    \n",
    "    # Calculate the form for the last 5 games, excluding the current match\n",
    "    df_new.loc[home_indices, 'HomeTeamRecentForm'] = df_new.loc[home_indices, 'HomeTeamPoints'].diff().fillna(0).rolling(window=6, min_periods=1).sum().shift(fill_value=0)\n",
    "    df_new.loc[away_indices, 'AwayTeamRecentForm'] = df_new.loc[away_indices, 'AwayTeamPoints'].diff().fillna(0).rolling(window=6, min_periods=1).sum().shift(fill_value=0)\n",
    "\n",
    "# Display the head of the dataframe to verify\n",
    "df_new[['Date', 'HomeTeam', 'HomeTeamPoints', 'HomeTeamRecentForm', 'AwayTeam', 'AwayTeamPoints', 'AwayTeamRecentForm']].tail(10)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8213f2396de7d7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, make_scorer, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Selecting features and target\n",
    "features = ['HomeTeamEncoded', 'AwayTeamEncoded', 'HomeTeamAvgGoals', 'AwayTeamAvgGoals', 'HomeTeamPoints', 'AwayTeamPoints', 'HomeTeamRecentForm', 'AwayTeamRecentForm',\n",
    "    'HomeTeamWinPercentage', 'AwayTeamWinPercentage']\n",
    "X = df_new[features]\n",
    "y = df_new['FTR']\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Addressing Class Imbalance\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_smote)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "    \n",
    "}\n",
    "grid_search = GridSearchCV(model, param_grid, cv=3, scoring='f1_macro')\n",
    "grid_search.fit(X_train_scaled, y_train_smote)\n",
    "\n",
    "# Best Model Evaluation\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "print(classification_report(y_test, y_pred))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67b825783be054c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert odds to implied probabilities\n",
    "for bookmaker in ['B365', 'BW', 'IW', 'WH']:\n",
    "    for outcome in ['H', 'D', 'A']:\n",
    "        odds_col = f'{bookmaker}{outcome}'\n",
    "        prob_col = f'ImpliedProb{bookmaker}{outcome}'\n",
    "        df_new[prob_col] = 1 / df_new[odds_col]\n",
    "\n",
    "# Normalize implied probabilities\n",
    "for bookmaker in ['B365', 'BW', 'IW', 'WH']:\n",
    "    prob_cols = [f'ImpliedProb{bookmaker}{outcome}' for outcome in ['H', 'D', 'A']]\n",
    "    df_new[prob_cols] = df_new[prob_cols].div(df_new[prob_cols].sum(axis=1), axis=0)\n",
    "\n",
    "# Include Asian Handicap odds\n",
    "df_new['AsianHandicapHome'] = df_new['B365AHH']  # Example for Bet365 Asian Handicap Home\n",
    "df_new['AsianHandicapAway'] = df_new['B365AHA']  # Example for Bet365 Asian Handicap Away\n",
    "\n",
    "# Include match statistics as features\n",
    "match_stats_features = ['HS', 'AS', 'HST', 'AST', 'HC', 'AC', 'HF', 'AF', 'HY', 'AY', 'HR', 'AR']\n",
    "df_new[match_stats_features] = df_new[match_stats_features].fillna(df_new[match_stats_features].median())\n",
    "\n",
    "# Your features now include odds, Asian handicaps, and match statistics\n",
    "features.extend([f'ImpliedProb{bookmaker}{outcome}' for bookmaker in ['B365', 'BW', 'IW', 'WH'] for outcome in ['H', 'D', 'A']])\n",
    "features.extend(['AsianHandicapHome', 'AsianHandicapAway'])\n",
    "features.extend(match_stats_features)\n",
    "\n",
    "# Now use these features in your model training and evaluation\n",
    "# ...\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f7377861a47ccb7"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1f9cf34a96ba28d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Define a list with models to evaluate\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression(random_state=42)),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=42)),\n",
    "    ('Support Vector Machine', SVC(random_state=42)),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier()),\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "# Record model performance\n",
    "model_performance = {}\n",
    "\n",
    "# Loop through models\n",
    "for name, model in models:\n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train_smote)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Record performance\n",
    "    model_performance[name] = classification_report(y_test, y_pred, output_dict=True)\n",
    "\n",
    "# Print the performance for each model\n",
    "for name, performance in model_performance.items():\n",
    "    print(f\"Model: {name}\")\n",
    "    print(pd.DataFrame(performance).transpose())\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4131bd69bc7612"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model using xgboost, lightgbm and catboost"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91c360297ddea660"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit label encoder and return encoded labels\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the dataset into training and testing sets with the encoded target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)\n",
    "\n",
    "# Now continue with training and evaluating the models as before\n",
    "classifiers = {\n",
    "    \"XGBoost\": XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, verbosity=1, objective='multi:softprob'),\n",
    "    \"LightGBM\": LGBMClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, verbose=-1),\n",
    "    \"CatBoost\": CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, loss_function='MultiClass', verbose=False)\n",
    "}\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "for classifier_name, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    evaluation_results[classifier_name] = report\n",
    "\n",
    "# Display the results\n",
    "for classifier_name, report in evaluation_results.items():\n",
    "    print(f\"Classifier: {classifier_name}\")\n",
    "    print(pd.DataFrame(report).transpose())\n",
    "    print(\"\\n\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a74a7b967d9eb287"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ensemble model with voting classifier"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84a7a7b1e6dd7b2d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import VotingClassifier\n",
    "# from sklearn.metrics import classification_report\n",
    "# \n",
    "# # Initialize the models\n",
    "# log_reg = LogisticRegression(random_state=42)\n",
    "# dec_tree = DecisionTreeClassifier(random_state=42)\n",
    "# svc = SVC(probability=True, random_state=42)\n",
    "# knn = KNeighborsClassifier()\n",
    "# rand_forest = RandomForestClassifier(random_state=42)\n",
    "# grad_boost = GradientBoostingClassifier(random_state=42)\n",
    "# xgb = XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, verbosity=1, objective='multi:softprob')\n",
    "# lgbm = LGBMClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, verbose=-1)\n",
    "# cat_boost = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=6, loss_function='MultiClass', verbose=False)\n",
    "# \n",
    "# # Define a list called `estimators` containing tuples of the form (name, model)\n",
    "# estimators = [\n",
    "#     ('Logistic Regression', log_reg),\n",
    "#     ('Decision Tree', dec_tree),\n",
    "#     ('Support Vector Machine', svc),\n",
    "#     ('K-Nearest Neighbors', knn),\n",
    "#     ('Random Forest', rand_forest),\n",
    "#     ('Gradient Boosting', grad_boost),\n",
    "#     ('XGBoost', xgb),\n",
    "#     ('LightGBM', lgbm),\n",
    "#     ('CatBoost', cat_boost)\n",
    "# ]\n",
    "# \n",
    "# label_encoder = LabelEncoder()\n",
    "# y_train_smote_encoded = label_encoder.fit_transform(y_train_smote)\n",
    "# y_test_encoded = label_encoder.transform(y_test)\n",
    "# \n",
    "# # Initialize voting classifier with 'soft' voting\n",
    "# voting_clf = VotingClassifier(estimators=estimators, voting='soft')\n",
    "# \n",
    "# # Fit the model on the TRAINING data that has been resampled and scaled\n",
    "# voting_clf.fit(X_train_scaled, y_train_smote)\n",
    "# \n",
    "# # Evaluate the model on the ORIGINAL TEST data (which has been scaled but not resampled)\n",
    "# y_pred = voting_clf.predict(X_test_scaled)\n",
    "# print(classification_report(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "262f6102bee16711"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
    "# print(\"y_train_encoded shape:\", y_train_encoded.shape)\n",
    "# print(\"X_test_scaled shape:\", X_test_scaled.shape)\n",
    "# print(\"y_test_encoded shape:\", y_test_encoded.shape)\n",
    "# \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cfd8eedda2b5f7f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# \n",
    "# # Initialize the label encoder\n",
    "# label_encoder = LabelEncoder()\n",
    "# \n",
    "# # Apply label encoding to the training and testing target variables separately\n",
    "# y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "# y_test_encoded = label_encoder.transform(y_test)\n",
    "# \n",
    "# # Now verify the shapes again\n",
    "# print(\"X_train_scaled shape:\", X_train_scaled.shape)\n",
    "# print(\"y_train_encoded shape:\", y_train_encoded.shape)\n",
    "# print(\"X_test_scaled shape:\", X_test_scaled.shape)\n",
    "# print(\"y_test_encoded shape:\", y_test_encoded.shape)\n",
    "# \n",
    "# # If the shapes now match, you can proceed with training the voting classifier\n",
    "# if X_train_scaled.shape[0] == y_train_encoded.shape[0] and X_test_scaled.shape[0] == y_test_encoded.shape[0]:\n",
    "#     voting_clf.fit(X_train_scaled, y_train_encoded)\n",
    "#     y_pred = voting_clf.predict(X_test_scaled)\n",
    "#     print(classification_report(y_test_encoded, y_pred))\n",
    "# else:\n",
    "#     print(\"Mismatch in dataset dimensions, please check data preprocessing steps.\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d58cec3a0653c032"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_new.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83885c5ce6e5ce31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from hyperopt import hp, tpe, Trials, fmin, STATUS_OK,space_eval\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Feature Engineering\n",
    "# Ensure that this step is already done and df_new contains the engineered features\n",
    "\n",
    "\n",
    "# Assuming 'df_new' is your DataFrame and 'E0' is in the 'Div'      \n",
    "pd.get_dummies(df_new, columns=['Div'], drop_first=True)\n",
    "\n",
    "\n",
    "# Hyperparameter Space\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 100, 1000, 50),\n",
    "    'max_depth': hp.choice('max_depth', np.arange(3, 11, dtype=int)),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 9, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "    'subsample': hp.uniform('subsample', 0.7, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.7, 1)\n",
    "}\n",
    "\n",
    "def objective(params):\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    clf = XGBClassifier(**params, use_label_encoder=False, objective='multi:softprob', eval_metric='mlogloss')\n",
    "    score = cross_val_score(clf, X_train_scaled, y_train, scoring='accuracy', cv=StratifiedKFold(5)).mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# List of categorical column names\n",
    "categorical_cols = ['HomeTeam', 'AwayTeam']  # Add all your categorical columns here\n",
    "\n",
    "# Apply label encoding to each categorical column\n",
    "for col in categorical_cols:\n",
    "    df_new[col] = label_encoder.fit_transform(df_new[col])\n",
    "\n",
    "# saving the label encoder\n",
    "import pickle\n",
    "pickle.dump(label_encoder, open('new_label_encoder.pkl', 'wb'))\n",
    "# Encoding target variable\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Scaling features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Run Hyperparameter Optimization\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=10, trials=trials)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = space_eval(space, best)\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Initialize model with best parameters\n",
    "model = XGBClassifier(**best_params, use_label_encoder=False, objective='multi:softprob', eval_metric='mlogloss')\n",
    "model.fit(X_train_scaled, y_train_encoded)\n",
    "\n",
    "# Prediction and Evaluation\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)\n",
    "print(classification_report(y_test_encoded, y_pred))\n",
    "\n",
    "\n",
    "# Calculate ROC AUC Score\n",
    "try:\n",
    "    roc_auc = roc_auc_score(y_test_encoded, y_pred_proba, multi_class='ovr')\n",
    "    print(f\"ROC AUC: {roc_auc}\")\n",
    "except ValueError as e:\n",
    "    print(\"Error calculating ROC AUC:\", e)\n",
    "\n",
    "# Other metrics\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "precision = precision_score(y_test_encoded, y_pred, average='macro')\n",
    "recall = recall_score(y_test_encoded, y_pred, average='macro')\n",
    "f1 = f1_score(y_test_encoded, y_pred, average='macro')\n",
    "roc_auc = roc_auc_score(y_test_encoded, y_pred_proba, multi_class='ovo')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"ROC AUC: {roc_auc}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a036067d71a1760"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: name 'X_train_scaled' is not defined\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 30\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# Run hyperparameter tuning\u001B[39;00m\n\u001B[0;32m     29\u001B[0m trials \u001B[38;5;241m=\u001B[39m Trials()\n\u001B[1;32m---> 30\u001B[0m best \u001B[38;5;241m=\u001B[39m \u001B[43mfmin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mobjective\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mspace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mspace\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malgo\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtpe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msuggest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_evals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m15\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrials\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m# Get best hyperparameters\u001B[39;00m\n\u001B[0;32m     33\u001B[0m best_params \u001B[38;5;241m=\u001B[39m space_eval(space, best)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\fmin.py:540\u001B[0m, in \u001B[0;36mfmin\u001B[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001B[0m\n\u001B[0;32m    537\u001B[0m     fn \u001B[38;5;241m=\u001B[39m __objective_fmin_wrapper(fn)\n\u001B[0;32m    539\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m allow_trials_fmin \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(trials, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfmin\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m--> 540\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrials\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfmin\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    541\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    542\u001B[0m \u001B[43m        \u001B[49m\u001B[43mspace\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    543\u001B[0m \u001B[43m        \u001B[49m\u001B[43malgo\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43malgo\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    544\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_evals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_evals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    545\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    546\u001B[0m \u001B[43m        \u001B[49m\u001B[43mloss_threshold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_threshold\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    547\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_queue_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_queue_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    548\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrstate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrstate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    549\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpass_expr_memo_ctrl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpass_expr_memo_ctrl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    550\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    551\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcatch_eval_exceptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcatch_eval_exceptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    552\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_argmin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_argmin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    553\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshow_progressbar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshow_progressbar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    554\u001B[0m \u001B[43m        \u001B[49m\u001B[43mearly_stop_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mearly_stop_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    555\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrials_save_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrials_save_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    556\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    558\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m trials \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    559\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(trials_save_file):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\base.py:671\u001B[0m, in \u001B[0;36mTrials.fmin\u001B[1;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001B[0m\n\u001B[0;32m    666\u001B[0m \u001B[38;5;66;03m# -- Stop-gap implementation!\u001B[39;00m\n\u001B[0;32m    667\u001B[0m \u001B[38;5;66;03m#    fmin should have been a Trials method in the first place\u001B[39;00m\n\u001B[0;32m    668\u001B[0m \u001B[38;5;66;03m#    but for now it's still sitting in another file.\u001B[39;00m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfmin\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m fmin\n\u001B[1;32m--> 671\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfmin\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    672\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    673\u001B[0m \u001B[43m    \u001B[49m\u001B[43mspace\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    674\u001B[0m \u001B[43m    \u001B[49m\u001B[43malgo\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43malgo\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    675\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_evals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_evals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    676\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    677\u001B[0m \u001B[43m    \u001B[49m\u001B[43mloss_threshold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mloss_threshold\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    678\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    679\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrstate\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrstate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    680\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    681\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_queue_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_queue_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    682\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_trials_fmin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# -- prevent recursion\u001B[39;49;00m\n\u001B[0;32m    683\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpass_expr_memo_ctrl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpass_expr_memo_ctrl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    684\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcatch_eval_exceptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcatch_eval_exceptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    685\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_argmin\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_argmin\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    686\u001B[0m \u001B[43m    \u001B[49m\u001B[43mshow_progressbar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshow_progressbar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    687\u001B[0m \u001B[43m    \u001B[49m\u001B[43mearly_stop_fn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mearly_stop_fn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    688\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrials_save_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrials_save_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    689\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\fmin.py:586\u001B[0m, in \u001B[0;36mfmin\u001B[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001B[0m\n\u001B[0;32m    583\u001B[0m rval\u001B[38;5;241m.\u001B[39mcatch_eval_exceptions \u001B[38;5;241m=\u001B[39m catch_eval_exceptions\n\u001B[0;32m    585\u001B[0m \u001B[38;5;66;03m# next line is where the fmin is actually executed\u001B[39;00m\n\u001B[1;32m--> 586\u001B[0m \u001B[43mrval\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexhaust\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    588\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_argmin:\n\u001B[0;32m    589\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(trials\u001B[38;5;241m.\u001B[39mtrials) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\fmin.py:364\u001B[0m, in \u001B[0;36mFMinIter.exhaust\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    362\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mexhaust\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    363\u001B[0m     n_done \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrials)\n\u001B[1;32m--> 364\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_evals\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mn_done\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mblock_until_done\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43masynchronous\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    365\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrials\u001B[38;5;241m.\u001B[39mrefresh()\n\u001B[0;32m    366\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\fmin.py:300\u001B[0m, in \u001B[0;36mFMinIter.run\u001B[1;34m(self, N, block_until_done)\u001B[0m\n\u001B[0;32m    297\u001B[0m     time\u001B[38;5;241m.\u001B[39msleep(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpoll_interval_secs)\n\u001B[0;32m    298\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    299\u001B[0m     \u001B[38;5;66;03m# -- loop over trials and do the jobs directly\u001B[39;00m\n\u001B[1;32m--> 300\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mserial_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    302\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrials\u001B[38;5;241m.\u001B[39mrefresh()\n\u001B[0;32m    303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrials_save_file \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\fmin.py:178\u001B[0m, in \u001B[0;36mFMinIter.serial_evaluate\u001B[1;34m(self, N)\u001B[0m\n\u001B[0;32m    176\u001B[0m ctrl \u001B[38;5;241m=\u001B[39m base\u001B[38;5;241m.\u001B[39mCtrl(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrials, current_trial\u001B[38;5;241m=\u001B[39mtrial)\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 178\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdomain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspec\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctrl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    179\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    180\u001B[0m     logger\u001B[38;5;241m.\u001B[39merror(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjob exception: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mstr\u001B[39m(e))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hyperopt\\base.py:892\u001B[0m, in \u001B[0;36mDomain.evaluate\u001B[1;34m(self, config, ctrl, attach_attachments)\u001B[0m\n\u001B[0;32m    883\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    884\u001B[0m     \u001B[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001B[39;00m\n\u001B[0;32m    885\u001B[0m     \u001B[38;5;66;03m#    either into the pyll part (self.expr)\u001B[39;00m\n\u001B[0;32m    886\u001B[0m     \u001B[38;5;66;03m#    or the normal Python part (self.fn)\u001B[39;00m\n\u001B[0;32m    887\u001B[0m     pyll_rval \u001B[38;5;241m=\u001B[39m pyll\u001B[38;5;241m.\u001B[39mrec_eval(\n\u001B[0;32m    888\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexpr,\n\u001B[0;32m    889\u001B[0m         memo\u001B[38;5;241m=\u001B[39mmemo,\n\u001B[0;32m    890\u001B[0m         print_node_on_error\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrec_eval_print_node_on_error,\n\u001B[0;32m    891\u001B[0m     )\n\u001B[1;32m--> 892\u001B[0m     rval \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpyll_rval\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    894\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(rval, (\u001B[38;5;28mfloat\u001B[39m, \u001B[38;5;28mint\u001B[39m, np\u001B[38;5;241m.\u001B[39mnumber)):\n\u001B[0;32m    895\u001B[0m     dict_rval \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mfloat\u001B[39m(rval), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstatus\u001B[39m\u001B[38;5;124m\"\u001B[39m: STATUS_OK}\n",
      "Cell \u001B[1;32mIn[1], line 25\u001B[0m, in \u001B[0;36mobjective\u001B[1;34m(params)\u001B[0m\n\u001B[0;32m     23\u001B[0m params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124miterations\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(params[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124miterations\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     24\u001B[0m clf \u001B[38;5;241m=\u001B[39m CatBoostClassifier(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams, loss_function\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMultiClass\u001B[39m\u001B[38;5;124m'\u001B[39m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m---> 25\u001B[0m score \u001B[38;5;241m=\u001B[39m cross_val_score(clf, \u001B[43mX_train_scaled\u001B[49m, y_train, scoring\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m, cv\u001B[38;5;241m=\u001B[39mStratifiedKFold(\u001B[38;5;241m10\u001B[39m))\u001B[38;5;241m.\u001B[39mmean()\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m-\u001B[39mscore, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstatus\u001B[39m\u001B[38;5;124m'\u001B[39m: STATUS_OK}\n",
      "\u001B[1;31mNameError\u001B[0m: name 'X_train_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "# catboost hyperparameter tuning\n",
    "from hyperopt import hp, tpe, Trials, fmin, STATUS_OK,space_eval\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from catboost import CatBoostClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Feature Engineering\n",
    "# Ensure that this step is already done and df_new contains the engineered features\n",
    "\n",
    "# Define hyperparameter space\n",
    "space = {\n",
    "    'iterations': hp.quniform('iterations', 100, 1000, 50),\n",
    "    'depth': hp.choice('depth', np.arange(3, 11, dtype=int)),\n",
    "    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n",
    "    'l2_leaf_reg': hp.uniform('l2_leaf_reg', 1, 10),\n",
    "    'bagging_temperature': hp.uniform('bagging_temperature', 0, 1),\n",
    "    'random_strength': hp.uniform('random_strength', 0, 1)\n",
    "}\n",
    "\n",
    "def objective(params):\n",
    "    params['iterations'] = int(params['iterations'])\n",
    "    clf = CatBoostClassifier(**params, loss_function='MultiClass', verbose=False)\n",
    "    score = cross_val_score(clf, X_train_scaled, y_train, scoring='accuracy', cv=StratifiedKFold(3)).mean()\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n",
    "# Run hyperparameter tuning\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=15, trials=trials)\n",
    "\n",
    "# Get best hyperparameters\n",
    "best_params = space_eval(space, best)\n",
    "\n",
    "# Initialize model with best parameters\n",
    "model = CatBoostClassifier(**best_params, loss_function='MultiClass', eval_metric='MultiClass', verbose=False)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train_scaled, y_train_encoded)\n",
    "\n",
    "# Prediction and Evaluation\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_pred_proba = model.predict_proba(X_test_scaled)\n",
    "print(classification_report(y_test_encoded, y_pred))\n",
    "\n",
    "# Calculate ROC AUC Score\n",
    "try:\n",
    "    roc_auc = roc_auc_score(y_test_encoded, y_pred_proba, multi_class='ovr')\n",
    "    print(f\"ROC AUC: {roc_auc}\")\n",
    "except ValueError as e:\n",
    "    print(\"Error calculating ROC AUC:\", e)\n",
    "\n",
    "# Other metrics\n",
    "accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "precision = precision_score(y_test_encoded, y_pred, average='macro')\n",
    "recall = recall_score(y_test_encoded, y_pred, average='macro')\n",
    "f1 = f1_score(y_test_encoded, y_pred, average='macro')\n",
    "roc_auc = roc_auc_score(y_test_encoded, y_pred_proba, multi_class='ovo')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"ROC AUC: {roc_auc}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-23T17:14:06.488433700Z",
     "start_time": "2023-12-23T17:14:03.074112Z"
    }
   },
   "id": "bc04e9c361caa1ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### saving the catboost model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23eae0e0fdbd4893"
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "# save the encoder and scaler\n",
    "import pickle\n",
    "pickle.dump(label_encoder, open('label_encoder.pkl', 'wb'))\n",
    "pickle.dump(scaler, open('scaler.pkl', 'wb'))\n",
    "\n",
    "# save the model as a pickle file\n",
    "pickle.dump(model, open('catboost_model.pkl', 'wb'))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T19:53:19.210263600Z",
     "start_time": "2023-12-21T19:53:19.187217900Z"
    }
   },
   "id": "544932f955096ff"
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6862745098039216\n"
     ]
    }
   ],
   "source": [
    "# load the model from disk\n",
    "import pickle\n",
    "loaded_model = pickle.load(open('catboost_model.pkl', 'rb'))\n",
    "result = loaded_model.score(X_test_scaled, y_test_encoded)\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T19:53:49.015951400Z",
     "start_time": "2023-12-21T19:53:48.983604100Z"
    }
   },
   "id": "9f0aafe2bbd25fb9"
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load the saved objects\n",
    "loaded_label_encoder = pickle.load(open('label_encoder.pkl', 'rb'))\n",
    "loaded_scaler = pickle.load(open('scaler.pkl', 'rb'))\n",
    "loaded_model = pickle.load(open('catboost_model.pkl', 'rb'))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T19:57:14.077229900Z",
     "start_time": "2023-12-21T19:57:14.046126800Z"
    }
   },
   "id": "313d95bb989a6d1e"
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HomeTeamEncoded', 'AwayTeamEncoded', 'HomeTeamAvgGoals', 'AwayTeamAvgGoals', 'HomeTeamPoints', 'AwayTeamPoints', 'HomeTeamRecentForm', 'AwayTeamRecentForm', 'HomeTeamWinPercentage', 'AwayTeamWinPercentage', 'ImpliedProbB365H', 'ImpliedProbB365D', 'ImpliedProbB365A', 'ImpliedProbBWH', 'ImpliedProbBWD', 'ImpliedProbBWA', 'ImpliedProbIWH', 'ImpliedProbIWD', 'ImpliedProbIWA', 'ImpliedProbWHH', 'ImpliedProbWHD', 'ImpliedProbWHA', 'AsianHandicapHome', 'AsianHandicapAway', 'HS', 'AS', 'HST', 'AST', 'HC', 'AC', 'HF', 'AF', 'HY', 'AY', 'HR', 'AR']\n"
     ]
    }
   ],
   "source": [
    "print(features)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T20:11:56.633343800Z",
     "start_time": "2023-12-21T20:11:56.603681700Z"
    }
   },
   "id": "b9a30460aa4eaee6"
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team name 'Chelsea' not recognized.\n",
      "Team name 'Luton' not recognized.\n",
      "Predicted Result: Invalid team name(s)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load the saved model and label encoder\n",
    "model = pickle.load(open('catboost_model.pkl', 'rb'))\n",
    "label_encoder = pickle.load(open('label_encoder.pkl', 'rb'))\n",
    "\n",
    "# Function to encode team names\n",
    "def encode_team(team_name):\n",
    "    try:\n",
    "        return label_encoder.transform([team_name])[0]\n",
    "    except ValueError:\n",
    "        print(f\"Team name '{team_name}' not recognized.\")\n",
    "        return -1  # Or handle unknown teams differently\n",
    "\n",
    "# Function to make a prediction with only home and away team names\n",
    "def make_prediction(home_team, away_team):\n",
    "    # Encode team names\n",
    "    home_team_encoded = encode_team(home_team)\n",
    "    away_team_encoded = encode_team(away_team)\n",
    "\n",
    "    if home_team_encoded == -1 or away_team_encoded == -1:\n",
    "        return \"Invalid team name(s)\"\n",
    "\n",
    "    # Creating a feature array with only team names\n",
    "    features_array = np.array([home_team_encoded, away_team_encoded]).reshape(1, -1)\n",
    "\n",
    "    # Predicting the result\n",
    "    prediction = model.predict(features_array)\n",
    "    return prediction\n",
    "\n",
    "# Example usage\n",
    "home_team = input(\"Enter Home Team: \")\n",
    "away_team = input(\"Enter Away Team: \")\n",
    "\n",
    "predicted_result = make_prediction(home_team, away_team)\n",
    "print(\"Predicted Result:\", predicted_result)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T20:18:47.247124100Z",
     "start_time": "2023-12-21T20:18:27.002610200Z"
    }
   },
   "id": "22c18a4d80ffea4"
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming 'df_new' contains the correct team names in 'HomeTeam' and 'AwayTeam' columns\n",
    "all_teams = pd.concat([df_new['HomeTeam'], df_new['AwayTeam']]).unique()\n",
    "label_encoder_n= LabelEncoder()\n",
    "label_encoder_n.fit(all_teams)\n",
    "\n",
    "# Save the re-trained label encoder\n",
    "pickle.dump(label_encoder_n, open('new_label_encoder.pkl', 'wb'))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T20:26:12.516402800Z",
     "start_time": "2023-12-21T20:26:12.479672100Z"
    }
   },
   "id": "3646d3c52a3da8f"
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n"
     ]
    }
   ],
   "source": [
    "print(label_encoder_n.classes_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T20:26:47.043880300Z",
     "start_time": "2023-12-21T20:26:47.007598200Z"
    }
   },
   "id": "e732c3960c92bb9f"
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Result: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dagbo_b40tnyc\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:153: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load the new label encoder and CatBoost model\n",
    "label_encoder_n = pickle.load(open('new_label_encoder.pkl', 'rb'))\n",
    "model = pickle.load(open('catboost_model.pkl', 'rb'))\n",
    "\n",
    "# Function to make a prediction\n",
    "def predict_match(home_team, away_team):\n",
    "    try:\n",
    "        # Encode the team names using the new label encoder\n",
    "        home_team_encoded = label_encoder_n.transform([home_team])[0]\n",
    "        away_team_encoded = label_encoder_n.transform([away_team])[0]\n",
    "\n",
    "        # Placeholder for other features (use average or typical values)\n",
    "        other_features = np.array([0] * 18)  # Adjust the length according to your model's feature count\n",
    "\n",
    "        # Combine all features\n",
    "        features = np.append([home_team_encoded, away_team_encoded], other_features).reshape(1, -1)\n",
    "\n",
    "        # Make a prediction\n",
    "        prediction = model.predict(features)\n",
    "        return label_encoder_n.inverse_transform(prediction)[0]\n",
    "    except Exception as e:\n",
    "        return f\"Error during prediction: {e}\"\n",
    "\n",
    "# Example usage\n",
    "home_team = 7\n",
    "away_team = 7\n",
    "result = predict_match(home_team, away_team)\n",
    "print(f\"Predicted Result: {result}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T20:33:41.867888100Z",
     "start_time": "2023-12-21T20:33:41.833141200Z"
    }
   },
   "id": "d352960a971ee645"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "aeae9a367f05d0c2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
